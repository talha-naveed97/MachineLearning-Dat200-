{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "suburban-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Import modules\n",
    "#==============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron,LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "outer-indie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0         0\n",
      "age                0\n",
      "antenna_length     0\n",
      "blood_blueness     0\n",
      "chest_radius       0\n",
      "height             0\n",
      "salt_levels        0\n",
      "siblings           0\n",
      "skin_coarseness    0\n",
      "target             0\n",
      "dtype: int64\n",
      "       age  antenna_length  blood_blueness  chest_radius  height  salt_levels  \\\n",
      "0     66.0       51.000000           384.0     549.00000    82.5        0.345   \n",
      "1     63.0       87.460259           306.0     466.64467    75.3        0.234   \n",
      "2    120.0       87.460259           477.0     466.64467    82.2        0.882   \n",
      "3     69.0       39.000000           204.0      45.00000    60.3        0.771   \n",
      "4    132.0       87.460259           318.0     466.64467    70.8        0.411   \n",
      "..     ...             ...             ...           ...     ...          ...   \n",
      "455   72.0       39.000000           357.0     150.00000    66.9        0.615   \n",
      "456  114.0       75.000000           216.0     466.64467    94.8        0.840   \n",
      "457  183.0       87.460259           438.0     466.64467    93.6        1.617   \n",
      "458  123.0       87.460259           588.0     466.64467   119.4        1.353   \n",
      "459  126.0       87.460259           255.0     466.64467    93.6        1.146   \n",
      "\n",
      "     siblings  skin_coarseness  \n",
      "0         3.0            246.0  \n",
      "1         0.0            156.0  \n",
      "2        21.0            192.0  \n",
      "3         6.0            186.0  \n",
      "4        36.0            240.0  \n",
      "..        ...              ...  \n",
      "455       3.0            162.0  \n",
      "456      27.0            234.0  \n",
      "457      12.0            276.0  \n",
      "458      21.0            270.0  \n",
      "459      18.0            234.0  \n",
      "\n",
      "[460 rows x 8 columns]\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "455    0\n",
      "456    0\n",
      "457    1\n",
      "458    1\n",
      "459    0\n",
      "Name: target, Length: 460, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "print(data.isnull().sum())\n",
    "\n",
    "X = data.iloc[:, 1:9]\n",
    "y = data.iloc[:,9]\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "textile-syndication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_score = \n",
    "# best_f1score = 0\n",
    "# best_model = ''\n",
    "# best_model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "remarkable-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=3, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "historical-draft",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = []\n",
    "max_items = 50\n",
    "for x in range(int(len(X_train)/50) + 1):\n",
    "    size = (x+1)*50\n",
    "    subset_data = X_train[:size]\n",
    "    subset_target = y_train[:size]\n",
    "    subset = (subset_data,subset_target)\n",
    "    subsets.append(subset)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pacific-discount",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subsets[6][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tired-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "etas = [0.001,0.01,0.1,1,10,100,1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acute-aging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.001, 0.01, 0.1, 1, 10, 100, 1000]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intelligent-latin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(X_train)/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "front-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_highest = 0\n",
    "eta_highest = 0\n",
    "epoch_highest = 0\n",
    "subset_highest = 0\n",
    "c_highest = 0\n",
    "penalty_highest = ''\n",
    "best_model_name = ''\n",
    "best_f1 = 0\n",
    "best_depth = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "explicit-requirement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\malik\\miniconda3\\envs\\dat200\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n"
     ]
    }
   ],
   "source": [
    "for subset in subsets:\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(subset[0])\n",
    "    X_train_std = sc.transform(subset[0])\n",
    "    X_test_std = sc.transform(X_test)\n",
    "    for eta in etas:\n",
    "        for epochs in range(5,101,5):\n",
    "            ppn = Perceptron(max_iter=epochs, eta0=eta, random_state=1)\n",
    "            ppn.fit(X_train_std,subset[1])\n",
    "            y_pred = ppn.predict(X_test_std)\n",
    "            score = ppn.score(X_test_std, y_test)\n",
    "            #score = f1_score(y_test, y_pred)\n",
    "            if score > score_highest:\n",
    "                score_highest = score\n",
    "                eta_highest = eta\n",
    "                epoch_highest = epochs\n",
    "                subset_highest = len(subset[0])\n",
    "                best_model = ppn\n",
    "                best_standarizer = sc\n",
    "                c_highest = 0\n",
    "                penalty_highest = ''\n",
    "                best_model_name = 'Perceptron'\n",
    "                best_f1 = f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "stopped-refrigerator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7536231884057971\n",
      "0.01\n",
      "10\n",
      "50\n",
      "Perceptron\n",
      "\n",
      "0.6136363636363636\n"
     ]
    }
   ],
   "source": [
    "print(score_highest)\n",
    "print(eta_highest)\n",
    "print(epoch_highest)\n",
    "print(subset_highest)\n",
    "print(best_model_name)\n",
    "print(penalty_highest)\n",
    "print(best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "suburban-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppn_highest = 0\n",
    "# eta_highest = 0\n",
    "# epoch_highest = 0\n",
    "# subset_highest = 0\n",
    "\n",
    "# sc = StandardScaler()\n",
    "# sc.fit(X_train)\n",
    "# X_train_std = sc.transform(X_train)\n",
    "# X_test_std = sc.transform(X_test)\n",
    "# for eta in etas:\n",
    "#     for epochs in range(5,101,5):\n",
    "#         ppn = Perceptron(max_iter=epochs, eta0=eta, random_state=1)\n",
    "#         ppn.fit(X_train_std,subset[1])\n",
    "#         y_pred = ppn.predict(X_test_std)\n",
    "#         score = ppn.score(X_test_std, y_test)\n",
    "#         if score > ppn_highest:\n",
    "#             ppn_highest = score\n",
    "#             eta_highest = eta\n",
    "#             epoch_highest = epochs\n",
    "#             subset_highest = len(subset[0])\n",
    "#             best_ppn = ppn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "irish-strength",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "1.0\n",
      "10.0\n",
      "100.0\n",
      "1000.0\n",
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "1.0\n",
      "10.0\n",
      "100.0\n",
      "1000.0\n",
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "1.0\n",
      "10.0\n",
      "100.0\n",
      "1000.0\n",
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "1.0\n",
      "10.0\n",
      "100.0\n",
      "1000.0\n",
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "1.0\n",
      "10.0\n",
      "100.0\n",
      "1000.0\n",
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "1.0\n",
      "10.0\n",
      "100.0\n",
      "1000.0\n",
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "1.0\n",
      "10.0\n",
      "100.0\n",
      "1000.0\n",
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "1.0\n",
      "10.0\n",
      "100.0\n",
      "1000.0\n",
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "1.0\n",
      "10.0\n",
      "100.0\n",
      "1000.0\n",
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "1.0\n",
      "10.0\n",
      "100.0\n",
      "1000.0\n",
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "1.0\n",
      "10.0\n",
      "100.0\n",
      "1000.0\n",
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "1.0\n",
      "10.0\n",
      "100.0\n",
      "1000.0\n",
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "1.0\n",
      "10.0\n",
      "100.0\n",
      "1000.0\n",
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "1.0\n",
      "10.0\n",
      "100.0\n",
      "1000.0\n"
     ]
    }
   ],
   "source": [
    "## Logistic Regression\n",
    "penalties = ['l1','l2']\n",
    "for subset in subsets:\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(subset[0])\n",
    "    X_train_std = sc.transform(subset[0])\n",
    "    X_test_std = sc.transform(X_test)\n",
    "    for penalty in penalties:\n",
    "        for c in np.arange(-3, 4):\n",
    "            print (10.**c)\n",
    "            lr = LogisticRegression(C=10.**c, \n",
    "                                penalty=penalty,\n",
    "                                random_state=1,\n",
    "                                solver='liblinear',\n",
    "                                multi_class='auto')\n",
    "            lr.fit(X_train_std,subset[1])\n",
    "            y_pred = lr.predict(X_test_std)\n",
    "            score = lr.score(X_test_std, y_test)\n",
    "            #score = f1_score(y_test, y_pred)\n",
    "            if score > score_highest:\n",
    "                score_highest = score\n",
    "                eta_highest = 0\n",
    "                c_highest = 10.**c\n",
    "                epoch_highest = 0\n",
    "                subset_highest = len(subset[0])\n",
    "                best_model = lr\n",
    "                best_standarizer = sc\n",
    "                penalty_highest = penalty\n",
    "                best_model_name = 'LogisticRegression'\n",
    "                best_f1 = f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "blank-world",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7681159420289855\n",
      "0\n",
      "0\n",
      "200\n",
      "LogisticRegression\n",
      "l2\n",
      "0.6521739130434783\n"
     ]
    }
   ],
   "source": [
    "print(score_highest)\n",
    "print(eta_highest)\n",
    "print(epoch_highest)\n",
    "print(subset_highest)\n",
    "print(best_model_name)\n",
    "print(penalty_highest)\n",
    "print(best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "industrial-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset in subsets:\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(subset[0])\n",
    "    X_train_std = sc.transform(subset[0])\n",
    "    X_test_std = sc.transform(X_test)\n",
    "    for c in np.arange(-3, 4):\n",
    "        svc = SVC(kernel='linear', C=10.**c, random_state=1)\n",
    "        svc.fit(X_train_std,subset[1])\n",
    "        y_pred = svc.predict(X_test_std)\n",
    "        score = svc.score(X_test_std, y_test)\n",
    "        #score = f1_score(y_test, y_pred)\n",
    "        if score > score_highest:\n",
    "            score_highest = score\n",
    "            eta_highest = 0\n",
    "            c_highest = 10.**c\n",
    "            epoch_highest = 0\n",
    "            subset_highest = len(subset[0])\n",
    "            best_model = svc\n",
    "            best_standarizer = sc\n",
    "            penalty_highest = ''\n",
    "            best_model_name = 'SVM_linear'\n",
    "            best_f1 = f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adjustable-agreement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7681159420289855\n",
      "0\n",
      "0\n",
      "200\n",
      "LogisticRegression\n",
      "l2\n",
      "0.6521739130434783\n"
     ]
    }
   ],
   "source": [
    "print(score_highest)\n",
    "print(eta_highest)\n",
    "print(epoch_highest)\n",
    "print(subset_highest)\n",
    "print(best_model_name)\n",
    "print(penalty_highest)\n",
    "print(best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "early-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_gamma = np.arange(0.1, 0.6, 0.1)\n",
    "for subset in subsets:\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(subset[0])\n",
    "    X_train_std = sc.transform(subset[0])\n",
    "    X_test_std = sc.transform(X_test)\n",
    "    for c in np.arange(-3, 4):\n",
    "        for gm in params_gamma:\n",
    "            svc = SVC(kernel='rbf', C=10.**c, random_state=1, gamma=gm)\n",
    "            svc.fit(X_train_std,subset[1])\n",
    "            y_pred = svc.predict(X_test_std)\n",
    "            score = svc.score(X_test_std, y_test)\n",
    "            #score = f1_score(y_test, y_pred)\n",
    "            if score > score_highest:\n",
    "                score_highest = score\n",
    "                eta_highest = 0\n",
    "                c_highest = 10.**c\n",
    "                epoch_highest = 0\n",
    "                subset_highest = len(subset[0])\n",
    "                best_model = svc\n",
    "                best_standarizer = sc\n",
    "                penalty_highest = ''\n",
    "                best_model_name = 'SVM_non_linear'\n",
    "                best_f1 = f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caroline-preliminary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7681159420289855\n",
      "0\n",
      "0\n",
      "200\n",
      "LogisticRegression\n",
      "l2\n",
      "0.6521739130434783\n"
     ]
    }
   ],
   "source": [
    "print(score_highest)\n",
    "print(eta_highest)\n",
    "print(epoch_highest)\n",
    "print(subset_highest)\n",
    "print(best_model_name)\n",
    "print(penalty_highest)\n",
    "print(best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "shaped-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset in subsets:\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(subset[0])\n",
    "    X_train_std = sc.transform(subset[0])\n",
    "    X_test_std = sc.transform(X_test)\n",
    "    for depth in np.arange(1, 11):\n",
    "        dt = DecisionTreeClassifier(criterion='gini', \n",
    "                                      max_depth=depth, \n",
    "                                      random_state=1)\n",
    "        dt.fit(X_train_std,subset[1])\n",
    "        y_pred = dt.predict(X_test_std)\n",
    "        score = dt.score(X_test_std, y_test)\n",
    "        #score = f1_score(y_test, y_pred)\n",
    "        if score > score_highest:\n",
    "            score_highest = score\n",
    "            eta_highest = 0\n",
    "            c_highest = 10.**c\n",
    "            epoch_highest = 0\n",
    "            subset_highest = len(subset[0])\n",
    "            best_model = dt\n",
    "            best_standarizer = sc\n",
    "            penalty_highest = ''\n",
    "            best_model_name = 'Decision Tree'\n",
    "            best_f1 = f1_score(y_test, y_pred)\n",
    "            best_depth = depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sized-angola",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7681159420289855\n",
      "0\n",
      "0\n",
      "200\n",
      "LogisticRegression\n",
      "l2\n",
      "0.6521739130434783\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(score_highest)\n",
    "print(eta_highest)\n",
    "print(epoch_highest)\n",
    "print(subset_highest)\n",
    "print(best_model_name)\n",
    "print(penalty_highest)\n",
    "print(best_f1)\n",
    "print(best_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "interracial-electronics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "for subset in subsets:\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(subset[0])\n",
    "    X_train_std = sc.transform(subset[0])\n",
    "    X_test_std = sc.transform(X_test)\n",
    "    for estimator in np.arange(100, 1001,100):\n",
    "        print(estimator)\n",
    "        forest = RandomForestClassifier(n_estimators=estimator, random_state=1,\n",
    "                                n_jobs=-1)\n",
    "        forest.fit(X_train_std,subset[1])\n",
    "        y_pred = forest.predict(X_test_std)\n",
    "        score = forest.score(X_test_std, y_test)\n",
    "        #score = f1_score(y_test, y_pred)\n",
    "        if score > score_highest:\n",
    "            score_highest = score\n",
    "            eta_highest = 0\n",
    "            c_highest = 10.**c\n",
    "            epoch_highest = 0\n",
    "            subset_highest = len(subset[0])\n",
    "            best_model = dt\n",
    "            best_standarizer = sc\n",
    "            penalty_highest = ''\n",
    "            best_model_name = 'Forest'\n",
    "            best_f1 = f1_score(y_test, y_pred)\n",
    "            best_depth = estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "flush-penguin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7681159420289855\n",
      "0\n",
      "0\n",
      "200\n",
      "LogisticRegression\n",
      "l2\n",
      "0.6521739130434783\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(score_highest)\n",
    "print(eta_highest)\n",
    "print(epoch_highest)\n",
    "print(subset_highest)\n",
    "print(best_model_name)\n",
    "print(penalty_highest)\n",
    "print(best_f1)\n",
    "print(best_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "defined-cannon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "for subset in subsets:\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(subset[0])\n",
    "    X_train_std = sc.transform(subset[0])\n",
    "    X_test_std = sc.transform(X_test)\n",
    "    for classes in np.arange(2, 10,2):\n",
    "        print(classes)\n",
    "        knn = KNeighborsClassifier(n_neighbors=classes, \n",
    "                                       metric='minkowski',\n",
    "                                       n_jobs=-1)\n",
    "        knn.fit(X_train_std,subset[1])\n",
    "        y_pred = knn.predict(X_test_std)\n",
    "        score = knn.score(X_test_std, y_test)\n",
    "        #score = f1_score(y_test, y_pred)\n",
    "        if score > score_highest:\n",
    "            score_highest = score\n",
    "            eta_highest = 0\n",
    "            c_highest = 10.**c\n",
    "            epoch_highest = 0\n",
    "            subset_highest = len(subset[0])\n",
    "            best_model = dt\n",
    "            best_standarizer = sc\n",
    "            penalty_highest = ''\n",
    "            best_model_name = 'knn'\n",
    "            best_f1 = f1_score(y_test, y_pred)\n",
    "            best_depth = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "protected-marina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7681159420289855\n",
      "0\n",
      "0\n",
      "200\n",
      "LogisticRegression\n",
      "l2\n",
      "0.6521739130434783\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(score_highest)\n",
    "print(eta_highest)\n",
    "print(epoch_highest)\n",
    "print(subset_highest)\n",
    "print(best_model_name)\n",
    "print(penalty_highest)\n",
    "print(best_f1)\n",
    "print(best_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-julian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acceptable-grade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0\n",
      " 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0\n",
      " 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
      " 0 0 1 1 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0\n",
      " 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1\n",
      " 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1\n",
      " 1 0 1 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "data_test = pd.read_csv(\"test.csv\")\n",
    "data_test = data_test.iloc[:, 1:9]\n",
    "data_test_std = best_standarizer.transform(data_test)\n",
    "test_pred = best_model.predict(data_test_std)\n",
    "print(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "continued-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = ['Id','Predicted']\n",
    "data = {'Predicted':test_pred} \n",
    "\n",
    "df = pd.DataFrame(data) \n",
    "df.index.name = 'Id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "front-cement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>308 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Predicted\n",
       "Id            \n",
       "0            0\n",
       "1            0\n",
       "2            0\n",
       "3            0\n",
       "4            0\n",
       "..         ...\n",
       "303          1\n",
       "304          0\n",
       "305          0\n",
       "306          0\n",
       "307          0\n",
       "\n",
       "[308 rows x 1 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "progressive-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('my_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-residence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
